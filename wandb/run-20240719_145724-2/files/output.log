D:\thesis\sss\model_fixed2.py:419: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\cb\pytorch_1000000000000\work\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:455.)
  y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)
step 10600: train loss 3.8923, val loss 3.9358
GPU Memory before iteration 10600:
  Total: 4.29 GB
  Reserved: 2.89 GB
  Allocated: 1.99 GB
  Free: 1.40 GB
  Estimated memory for this iteration: 6.77 GB
iter 10600: loss 3.9733, time 20161.89ms, mfu -100.00%
Time stamp: 2024-07-19 14:57:46, Time since last checkpoint: 0:00:20
GPU Memory before iteration 10610:
  Total: 4.29 GB
  Reserved: 3.75 GB
  Allocated: 2.08 GB
  Free: 0.55 GB
  Estimated memory for this iteration: 6.77 GB
iter 10610: loss 3.7609, time 4028.78ms, mfu 1.88%
Time stamp: 2024-07-19 14:58:26, Time since last checkpoint: 0:00:40
GPU Memory before iteration 10620:
  Total: 4.29 GB
  Reserved: 3.75 GB
  Allocated: 2.08 GB
  Free: 0.55 GB
  Estimated memory for this iteration: 6.77 GB
iter 10620: loss 4.0612, time 4034.50ms, mfu 1.88%
Time stamp: 2024-07-19 14:59:06, Time since last checkpoint: 0:00:40
GPU Memory before iteration 10630:
  Total: 4.29 GB
  Reserved: 3.75 GB
  Allocated: 2.08 GB
  Free: 0.55 GB
  Estimated memory for this iteration: 6.77 GB
iter 10630: loss 4.3500, time 4141.24ms, mfu 1.88%
Time stamp: 2024-07-19 14:59:48, Time since last checkpoint: 0:00:41
GPU Memory before iteration 10640:
  Total: 4.29 GB
  Reserved: 3.75 GB
  Allocated: 2.08 GB
  Free: 0.55 GB
  Estimated memory for this iteration: 6.77 GB
iter 10640: loss 3.6559, time 4180.91ms, mfu 1.87%
Time stamp: 2024-07-19 15:00:30, Time since last checkpoint: 0:00:41
GPU Memory before iteration 10650:
  Total: 4.29 GB
  Reserved: 3.75 GB
  Allocated: 2.08 GB
  Free: 0.55 GB
  Estimated memory for this iteration: 6.77 GB
iter 10650: loss 3.6185, time 4138.48ms, mfu 1.87%
Time stamp: 2024-07-19 15:01:11, Time since last checkpoint: 0:00:40
Traceback (most recent call last):
  File "D:\thesis\sss\train_fixed.py", line 345, in <module>
    # backward pass, with gradient scaling if training in fp16
  File "C:\Users\Ronald\miniconda3\envs\torchenv\lib\site-packages\torch\_tensor.py", line 525, in backward
    torch.autograd.backward(
  File "C:\Users\Ronald\miniconda3\envs\torchenv\lib\site-packages\torch\autograd\__init__.py", line 267, in backward
    _engine_run_backward(
  File "C:\Users\Ronald\miniconda3\envs\torchenv\lib\site-packages\torch\autograd\graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
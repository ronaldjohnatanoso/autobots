D:\thesis\sss\model.py:64: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\cb\pytorch_1000000000000\work\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:455.)
  y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)
step 0: train loss 10.9997, val loss 10.9993
GPU Memory before iteration 0:
  Total: 4.29 GB
  Reserved: 2.74 GB
  Allocated: 0.51 GB
  Free: 1.56 GB
  Estimated memory for this iteration: 15.56 GB
iter 0: loss 10.9712, time 50290.50ms, mfu -100.00%
Time stamp: 2024-07-08 18:27:59, Time since last checkpoint: 0:00:50
GPU Memory before iteration 10:
  Total: 4.29 GB
  Reserved: 5.83 GB
  Allocated: 1.82 GB
  Free: -1.54 GB
  Estimated memory for this iteration: 15.56 GB
iter 10: loss 10.4469, time 46213.47ms, mfu 0.41%
Time stamp: 2024-07-08 18:35:15, Time since last checkpoint: 0:07:15
GPU Memory before iteration 20:
  Total: 4.29 GB
  Reserved: 5.83 GB
  Allocated: 1.82 GB
  Free: -1.54 GB
  Estimated memory for this iteration: 15.56 GB
iter 20: loss 9.7409, time 48555.80ms, mfu 0.41%
Time stamp: 2024-07-08 18:42:24, Time since last checkpoint: 0:07:09
GPU Memory before iteration 30:
  Total: 4.29 GB
  Reserved: 5.83 GB
  Allocated: 1.82 GB
  Free: -1.54 GB
  Estimated memory for this iteration: 15.56 GB
iter 30: loss 9.4217, time 51057.42ms, mfu 0.40%
Time stamp: 2024-07-08 18:49:48, Time since last checkpoint: 0:07:23
GPU Memory before iteration 40:
  Total: 4.29 GB
  Reserved: 5.83 GB
  Allocated: 1.82 GB
  Free: -1.54 GB
  Estimated memory for this iteration: 15.56 GB
iter 40: loss 9.1419, time 51154.16ms, mfu 0.40%
Time stamp: 2024-07-08 18:57:43, Time since last checkpoint: 0:07:54
GPU Memory before iteration 50:
  Total: 4.29 GB
  Reserved: 5.83 GB
  Allocated: 1.82 GB
  Free: -1.54 GB
  Estimated memory for this iteration: 15.56 GB
iter 50: loss 8.8292, time 51086.27ms, mfu 0.40%
Time stamp: 2024-07-08 19:05:38, Time since last checkpoint: 0:07:54
GPU Memory before iteration 60:
  Total: 4.29 GB
  Reserved: 5.83 GB
  Allocated: 1.82 GB
  Free: -1.54 GB
  Estimated memory for this iteration: 15.56 GB
iter 60: loss 8.6134, time 50892.89ms, mfu 0.39%
Time stamp: 2024-07-08 19:13:29, Time since last checkpoint: 0:07:51
GPU Memory before iteration 70:
  Total: 4.29 GB
  Reserved: 5.83 GB
  Allocated: 1.82 GB
  Free: -1.54 GB
  Estimated memory for this iteration: 15.56 GB
iter 70: loss 8.7093, time 49575.61ms, mfu 0.39%
Time stamp: 2024-07-08 19:21:07, Time since last checkpoint: 0:07:38
GPU Memory before iteration 80:
  Total: 4.29 GB
  Reserved: 5.83 GB
  Allocated: 1.82 GB
  Free: -1.54 GB
  Estimated memory for this iteration: 15.56 GB
iter 80: loss 8.4338, time 51268.99ms, mfu 0.39%
Time stamp: 2024-07-08 19:28:54, Time since last checkpoint: 0:07:46
GPU Memory before iteration 90:
  Total: 4.29 GB
  Reserved: 5.83 GB
  Allocated: 1.82 GB
  Free: -1.54 GB
  Estimated memory for this iteration: 15.56 GB
iter 90: loss 8.2224, time 46925.47ms, mfu 0.39%
Time stamp: 2024-07-08 19:36:43, Time since last checkpoint: 0:07:48
GPU Memory before iteration 100:
  Total: 4.29 GB
  Reserved: 5.83 GB
  Allocated: 1.82 GB
  Free: -1.54 GB
  Estimated memory for this iteration: 15.56 GB
iter 100: loss 8.0026, time 44248.93ms, mfu 0.39%
Time stamp: 2024-07-08 19:43:46, Time since last checkpoint: 0:07:03
GPU Memory before iteration 110:
  Total: 4.29 GB
  Reserved: 5.83 GB
  Allocated: 1.82 GB
  Free: -1.54 GB
  Estimated memory for this iteration: 15.56 GB
iter 110: loss 7.6608, time 51602.36ms, mfu 0.39%
Time stamp: 2024-07-08 19:51:16, Time since last checkpoint: 0:07:29
Traceback (most recent call last):
  File "D:\thesis\sss\train.py", line 340, in <module>
    scaler.scale(loss).backward()
  File "C:\Users\Ronald\miniconda3\envs\torchenv\lib\site-packages\torch\_tensor.py", line 525, in backward
    torch.autograd.backward(
  File "C:\Users\Ronald\miniconda3\envs\torchenv\lib\site-packages\torch\autograd\__init__.py", line 267, in backward
    _engine_run_backward(
  File "C:\Users\Ronald\miniconda3\envs\torchenv\lib\site-packages\torch\autograd\graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
D:\thesis\sss\model_fixed2.py:419: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\cb\pytorch_1000000000000\work\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:455.)
  y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)
step 10600: train loss 3.8923, val loss 3.9358
GPU Memory before iteration 10600:
  Total: 4.29 GB
  Reserved: 2.89 GB
  Allocated: 1.99 GB
  Free: 1.40 GB
  Estimated memory for this iteration: 6.77 GB
iter 10600: loss 3.9733, time 20631.14ms, mfu -100.00%
Time stamp: 2024-07-19 14:52:28, Time since last checkpoint: 0:00:20
GPU Memory before iteration 10610:
  Total: 4.29 GB
  Reserved: 3.75 GB
  Allocated: 2.08 GB
  Free: 0.55 GB
  Estimated memory for this iteration: 6.77 GB
iter 10610: loss 3.7603, time 4122.97ms, mfu 1.84%
Time stamp: 2024-07-19 14:53:11, Time since last checkpoint: 0:00:42
GPU Memory before iteration 10620:
  Total: 4.29 GB
  Reserved: 3.75 GB
  Allocated: 2.08 GB
  Free: 0.55 GB
  Estimated memory for this iteration: 6.77 GB
iter 10620: loss 4.0620, time 4140.45ms, mfu 1.84%
Time stamp: 2024-07-19 14:53:53, Time since last checkpoint: 0:00:42
Traceback (most recent call last):
  File "D:\thesis\sss\train_fixed.py", line 339, in <module>
    logits, loss = model(X, Y)
  File "C:\Users\Ronald\miniconda3\envs\torchenv\lib\site-packages\torch\nn\modules\module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\Ronald\miniconda3\envs\torchenv\lib\site-packages\torch\nn\modules\module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\thesis\sss\model_fixed2.py", line 539, in forward
    x = block(x)
  File "C:\Users\Ronald\miniconda3\envs\torchenv\lib\site-packages\torch\nn\modules\module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\Ronald\miniconda3\envs\torchenv\lib\site-packages\torch\nn\modules\module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\thesis\sss\model_fixed2.py", line 460, in forward
    x = x + self.mlp(self.ln_2(x))
  File "C:\Users\Ronald\miniconda3\envs\torchenv\lib\site-packages\torch\nn\modules\module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\Ronald\miniconda3\envs\torchenv\lib\site-packages\torch\nn\modules\module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\thesis\sss\model_fixed2.py", line 444, in forward
    x = self.gelu(x)
  File "C:\Users\Ronald\miniconda3\envs\torchenv\lib\site-packages\torch\nn\modules\module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\Ronald\miniconda3\envs\torchenv\lib\site-packages\torch\nn\modules\module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\Ronald\miniconda3\envs\torchenv\lib\site-packages\torch\nn\modules\activation.py", line 696, in forward
    return F.gelu(input, approximate=self.approximate)
KeyboardInterrupt